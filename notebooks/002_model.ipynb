{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa035ac",
   "metadata": {},
   "source": [
    "# Model Selection for Happiness Score Prediction\n",
    "\n",
    "This notebook evaluates and compares multiple regression algorithms ( Linear Regression, Random Forest, Gradient Boosting) to determine the most effective approach for predicting `happiness_score`. We use  metrics (MAE, RMSE) and feature‐importance analysis to guide our choice.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741b616",
   "metadata": {},
   "source": [
    "### Enviroment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454a18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory before change: c:\\Users\\Administrador\\Desktop\\workshop_03_kafka\\notebooks\n",
      "Current directory after change: c:\\Users\\Administrador\\Desktop\\workshop_03_kafka\n"
     ]
    }
   ],
   "source": [
    "print(\"Current directory before change:\", os.getcwd())\n",
    "\n",
    "try:\n",
    "    \n",
    "    os.chdir(\"../\")\n",
    "    print(\"Current directory after change:\", os.getcwd())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\"\"\n",
    "        FileNotFoundError - The specified directory does not exist or you are already in the root.\n",
    "        If the code already worked once, do not run it again.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc8340",
   "metadata": {},
   "source": [
    "### Load Combined Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97195f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:df shape: (781, 12)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/combined_happiness.csv')\n",
    "logger.info(f'df shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8096b3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>freedom</th>\n",
       "      <th>gdp_per_capita</th>\n",
       "      <th>generosity</th>\n",
       "      <th>happiness_rank</th>\n",
       "      <th>happiness_score</th>\n",
       "      <th>healthy_life_expectancy</th>\n",
       "      <th>social_support</th>\n",
       "      <th>trust_government_corruption</th>\n",
       "      <th>year</th>\n",
       "      <th>continent</th>\n",
       "      <th>gdp_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>0.66557</td>\n",
       "      <td>1.39651</td>\n",
       "      <td>0.29678</td>\n",
       "      <td>1</td>\n",
       "      <td>7.587</td>\n",
       "      <td>0.94143</td>\n",
       "      <td>1.34951</td>\n",
       "      <td>0.41978</td>\n",
       "      <td>2015</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.884604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iceland</td>\n",
       "      <td>0.62877</td>\n",
       "      <td>1.30232</td>\n",
       "      <td>0.43630</td>\n",
       "      <td>2</td>\n",
       "      <td>7.561</td>\n",
       "      <td>0.94784</td>\n",
       "      <td>1.40223</td>\n",
       "      <td>0.14145</td>\n",
       "      <td>2015</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.826152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>0.64938</td>\n",
       "      <td>1.32548</td>\n",
       "      <td>0.34139</td>\n",
       "      <td>3</td>\n",
       "      <td>7.527</td>\n",
       "      <td>0.87464</td>\n",
       "      <td>1.36058</td>\n",
       "      <td>0.48357</td>\n",
       "      <td>2015</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.803422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Norway</td>\n",
       "      <td>0.66973</td>\n",
       "      <td>1.45900</td>\n",
       "      <td>0.34699</td>\n",
       "      <td>4</td>\n",
       "      <td>7.522</td>\n",
       "      <td>0.88521</td>\n",
       "      <td>1.33095</td>\n",
       "      <td>0.36503</td>\n",
       "      <td>2015</td>\n",
       "      <td>Europe</td>\n",
       "      <td>1.941856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canada</td>\n",
       "      <td>0.63297</td>\n",
       "      <td>1.32629</td>\n",
       "      <td>0.45811</td>\n",
       "      <td>5</td>\n",
       "      <td>7.427</td>\n",
       "      <td>0.90563</td>\n",
       "      <td>1.32261</td>\n",
       "      <td>0.32957</td>\n",
       "      <td>2015</td>\n",
       "      <td>America</td>\n",
       "      <td>1.754164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  freedom  gdp_per_capita  generosity  happiness_rank  \\\n",
       "0  Switzerland  0.66557         1.39651     0.29678               1   \n",
       "1      Iceland  0.62877         1.30232     0.43630               2   \n",
       "2      Denmark  0.64938         1.32548     0.34139               3   \n",
       "3       Norway  0.66973         1.45900     0.34699               4   \n",
       "4       Canada  0.63297         1.32629     0.45811               5   \n",
       "\n",
       "   happiness_score  healthy_life_expectancy  social_support  \\\n",
       "0            7.587                  0.94143         1.34951   \n",
       "1            7.561                  0.94784         1.40223   \n",
       "2            7.527                  0.87464         1.36058   \n",
       "3            7.522                  0.88521         1.33095   \n",
       "4            7.427                  0.90563         1.32261   \n",
       "\n",
       "   trust_government_corruption  year continent  gdp_support  \n",
       "0                      0.41978  2015    Europe     1.884604  \n",
       "1                      0.14145  2015    Europe     1.826152  \n",
       "2                      0.48357  2015    Europe     1.803422  \n",
       "3                      0.36503  2015    Europe     1.941856  \n",
       "4                      0.32957  2015   America     1.754164  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d3c52",
   "metadata": {},
   "source": [
    "### Feature and Target Definition\n",
    "\n",
    "At this stage, we designate our curated set of predictors—including socio-economic metrics, engineered interaction terms, temporal and geographic features—as the feature matrix (“X”), and set `happiness_score` as the response vector (“y”). This clear separation of inputs and output lays the foundation for all subsequent model fitting and evaluation steps.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ba406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'freedom',\n",
    "    'gdp_per_capita',\n",
    "    'healthy_life_expectancy',\n",
    "    'social_support',\n",
    "    'generosity',\n",
    "    'trust_government_corruption',\n",
    "    'year',\n",
    "    'continent',\n",
    "    'gdp_support'\n",
    "]\n",
    "\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['happiness_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418a07b",
   "metadata": {},
   "source": [
    "### Split Data into Training and Testing Sets\n",
    "\n",
    "We partition the data into training (70%) and testing (30%) subsets to evaluate model performance on unseen data. A fixed random seed (`random_state=42`) ensures reproducibility of the split.\n",
    "\n",
    "**Purpose:**  \n",
    "- **Training set:** Used to fit model parameters  \n",
    "- **Testing set:** Used to assess predictive accuracy and generalization  \n",
    "\n",
    "This separation prevents overfitting and offers a reliable estimate of real‐world model performance.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56eb243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:X_train shape: (546, 9), X_test shape: (235, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "logger.info(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44266c",
   "metadata": {},
   "source": [
    "### One-Hot Encode `continent` and `year`\n",
    "\n",
    "To incorporate `continent` and `year` into our regression models, we convert these categorical features into binary indicator columns (one-hot encoding) and drop the first category to avoid multicollinearity. We then align the training and test matrices to ensure they share identical column sets, filling any missing indicators with zeros.\n",
    "\n",
    "**Why it matters:**  \n",
    "- Transforms non-numeric categories into numeric form for algorithm compatibility  \n",
    "- Prevents accidental feature mismatches between training and test sets  \n",
    "- Maintains consistent input dimensions for model fitting and prediction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504bc974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.get_dummies(X_train, columns=['continent','year'], drop_first=True)\n",
    "X_test_enc  = pd.get_dummies(X_test,  columns=['continent','year'], drop_first=True)\n",
    "X_train_enc, X_test_enc = X_train_enc.align(X_test_enc, join='left', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486221bb",
   "metadata": {},
   "source": [
    "### Train and Compare Multiple Regression Models\n",
    "\n",
    "We instantiate three regression algorithms—ordinary least squares, Lasso (with α=0.1), and Gradient Boosting (100 trees)—and wrap each in a pipeline that standardizes features before fitting. After training on the encoded training set, we generate predictions on the test set and compute key performance metrics (MAE, MSE, R²) for each model.\n",
    "\n",
    "**Key steps:**\n",
    "- **Model definitions:** Linear Regression, Lasso Regression, Gradient Boosting Regressor  \n",
    "- **Pipelines:** StandardScaler → model  \n",
    "- **Evaluation metrics:**  \n",
    "  - **MAE (Mean Absolute Error):** Average absolute prediction error  \n",
    "  - **MSE (Mean Squared Error):** Average squared prediction error  \n",
    "  - **R² (Coefficient of Determination):** Proportion of variance explained  \n",
    "\n",
    "**Interpretation:**  \n",
    "By comparing MAE, MSE, and R² across models, we identify which algorithm best balances bias and variance for predicting `happiness_score`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce2e71b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model comparison results:\n",
      "INFO:__main__:\n",
      "              model       MAE       MSE        R2\n",
      "0  LinearRegression  0.364853  0.246310  0.802746\n",
      "1             Lasso  0.423947  0.291309  0.766708\n",
      "2  GradientBoosting  0.338363  0.191572  0.846582\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Lasso': Lasso(alpha=0.1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train_enc, y_train)\n",
    "    y_pred = pipeline.predict(X_test_enc)\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'R2': r2_score(y_test, y_pred)\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "logger.info('Model comparison results:')\n",
    "logger.info(f'\\n{results_df}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be43f03",
   "metadata": {},
   "source": [
    "\n",
    "Based on test‐set results:\n",
    "\n",
    "- **Gradient Boosting** achieved the lowest MAE (0.338) and MSE (0.192), and the highest R² (0.847), indicating the best predictive accuracy and variance explanation.\n",
    "- **Linear Regression** performed well (MAE 0.365, MSE 0.246, R² 0.803), but lags behind Gradient Boosting in capturing non‐linear patterns.\n",
    "- **Lasso Regression** showed the weakest performance (MAE 0.424, MSE 0.291, R² 0.767), suggesting that its coefficient shrinkage may overly simplify relationships.\n",
    "\n",
    "**Conclusion:**  \n",
    "For now, Gradient Boosting is the optimal choice for predicting `happiness_score` in this setup, balancing low error with strong explanatory power.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5effc",
   "metadata": {},
   "source": [
    "### XGBoost Regression Pipeline\n",
    "\n",
    "Here we build and evaluate a pipeline that applies standard scaling followed by an XGBoost regressor:\n",
    "\n",
    "- **Pipeline components:**  \n",
    "  - **StandardScaler:** Normalizes features for consistent scale  \n",
    "  - **XGBRegressor:** Leverages gradient‐boosted trees to capture complex, non‐linear interactions  \n",
    "\n",
    "**Why XGBoost?**  \n",
    "Its tree‐based ensemble often outperforms linear methods by modeling intricate feature relationships and handling heterogeneity in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a5d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:XGBoost MAE: 0.3481, R²: 0.8332\n"
     ]
    }
   ],
   "source": [
    "pipeline_xgb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n",
    "pipeline_xgb.fit(X_train_enc, y_train)\n",
    "y_pred_xgb = pipeline_xgb.predict(X_test_enc)\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb  = r2_score(y_test, y_pred_xgb)\n",
    "logger.info(f'XGBoost MAE: {mae_xgb:.4f}, R²: {r2_xgb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04119c",
   "metadata": {},
   "source": [
    "### XGBoost Results vs. Gradient Boosting\n",
    "\n",
    "The XGBoost pipeline yielded:\n",
    "- **MAE:** 0.3481  \n",
    "- **R²:** 0.8332  \n",
    "\n",
    "This R² is slightly below the 0.8466 achieved by our earlier Gradient Boosting model. At this stage, Gradient Boosting remains the top performer in explaining variance.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0b0da",
   "metadata": {},
   "source": [
    "### Test with Random Forest\n",
    "This step fits a Random Forest model using the established scaling and training split, then logs MAE and R² on the test set. We will compare these results against Gradient Boosting and XGBoost to determine if Random Forest offers improved predictive performance for `happiness_score`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf0a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RF MAE: 0.3384, R²: 0.8466\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[feature_cols],\n",
    "    df['happiness_score'],\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_rf = pipeline\n",
    "pipeline_rf.fit(X_train_enc, y_train)\n",
    "y_pred = pipeline_rf.predict(X_test_enc)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2  = r2_score(y_test, y_pred)\n",
    "logger.info(f'RF MAE: {mae:.4f}, R²: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ac213",
   "metadata": {},
   "source": [
    "## Final Random Forest Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea398461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:RandomForest MAE: 0.3228, R²: 0.8516\n"
     ]
    }
   ],
   "source": [
    "pipeline_rf_final = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rf_final.fit(X_train_enc, y_train)\n",
    "y_pred_rf = pipeline_rf_final.predict(X_test_enc)\n",
    "\n",
    "# Puedes calcular métricas si quieres\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "logger.info(f'RandomForest MAE: {mae_rf:.4f}, R²: {r2_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df0047",
   "metadata": {},
   "source": [
    "### Final Model Selection: Random Forest Regression\n",
    "\n",
    "After evaluating multiple algorithms, the Random Forest pipeline achieved the best performance:\n",
    "\n",
    "- **MAE:** 0.3228  \n",
    "- **R²:** 0.8516  \n",
    "\n",
    "This R² surpasses both Gradient Boosting (0.8466) and XGBoost (0.8332), indicating that Random Forest provides the most accurate and robust predictions of `happiness_score`. We will use this model for our final analyses and forecasts.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f55c17",
   "metadata": {},
   "source": [
    "### Save Final Model to `model/model_random_forest.pkl`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beeb2ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved final RF model to model_random_forest.pkl\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(pipeline_rf_final, 'model/model_random_forest.pkl')\n",
    "logger.info('Saved final RF model to model_random_forest.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
